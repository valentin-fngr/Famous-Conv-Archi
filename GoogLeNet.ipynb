{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tf'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-6dc307728408>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tf'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from tf.keras.layers import Conv2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's build GoogleNet \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This architecture uses the Inception module, We will explain this module :)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Important notes about the first reading : </h3>\n",
    "<br>\n",
    "<ul> \n",
    "    <li>\n",
    "        <i>\" 1 × 1 convolutions have dual purpose: most critically, they are used mainly as dimension reduction modules to remove computational bottlenecks \"</i>\n",
    "    </li>\n",
    "    <li>\n",
    "        <i>The most straightforward way of improving the performance of deep neural networks is by increasing their size</i>\n",
    "    </li>\n",
    "    <li>\n",
    "        <i>Bigger size typically means a larger number of parameters, which makes the enlarged network more\n",
    "            prone to overfitting</i>\n",
    "    </li>\n",
    "    <li>\n",
    "        <i>Another drawback of uniformly increased network size is the dramatically increased use of computational resources</i>\n",
    "    </li>\n",
    "    <li>\n",
    "        <i> current incarnations of the Inception architecture are restricted to filter sizes 1×1,\n",
    "3×3 and 5×5, however this decision was based more on convenience rather than necessity</i>\n",
    "    <li>\n",
    "        <img src=\"inception.PNG\"/>\n",
    "    </li>\n",
    "    <li>\n",
    "        <i>Besides being used as reductions, they also include the use of rectified linear activation which makes them dual-purpose</i>\n",
    "    </li>\n",
    "    <li>\n",
    "        <i>occasional max-pooling layers with stride 2 to halve the resolution of the grid</i>\n",
    "    </li>\n",
    "    <li>\n",
    "        <img src=\"GoogleNetSummary.PNG\">\n",
    "    </li>\n",
    "    <li>\n",
    "        <i>\n",
    "            We have also\n",
    "used a deeper and wider Inception network, the quality of which was slightly inferior, but adding it\n",
    "to the ensemble seemed to improve the results marginally.\n",
    "        </i>\n",
    "    </li>\n",
    "</ul>\n",
    "\n",
    "<h2>Topology</h2>\n",
    "\n",
    "<ul>\n",
    "    <li><i>size of the receptive field in our network is 224×224 taking RGB color channels with mean subtraction</i></li>\n",
    "    <li><i>“#3×3 reduce” and “#5×5 reduce” stands for the number of 1×1 filters in the reduction</i></li>\n",
    "    <li><i>The network is 22 layers deep when counting only layers with\n",
    "        parameters (or 27 layers if we also count pooling)</i></li>\n",
    "    <li><i>The overall number of layers (independent building blocks) used for the construction of the network is about 100.</i></li>\n",
    "    <li><i>however the use of dropout remained\n",
    "        essential even after removing the fully connected layers</i></li>\n",
    "    <li><i>By adding auxiliary classifiers connected to\n",
    "these intermediate layers, we would expect to encourage discrimination in the lower stages in the\n",
    "classifier, increase the gradient signal that gets propagated back, and provide additional regularization</i></li>\n",
    "    <li>First extra Classifier : </li>\n",
    "    <img src=\"first_classifier.PNG\">\n",
    "    <li>Second extra classifier </li>\n",
    "    <img src=\"extra_classifier2.PNG\">\n",
    "    <li><i>An average pooling layer with 5×5 filter size and stride 3, resulting in an 4×4×512 output\n",
    "        for the (4a), and 4×4×528 for the (4d) stage</i></li>\n",
    "    <li><i>A 1×1 convolution with 128 filters for dimension reduction and rectified linear activation.</i></li>\n",
    "    <li><i>A fully connected layer with 1024 units and rectified linear activation.</i></li>\n",
    "    <li><i>A dropout layer with 70% ratio of dropped outputs</i></li>\n",
    "    <li><i>A linear layer with softmax loss as the classifier (predicting the same 1000 classes as the\n",
    "        main classifier, but removed at inference time).</i></li>\n",
    "    <img src=\"full_GoogleNet.PNG\">\n",
    "</ul>\n",
    "<h2>Training </h2>\n",
    "\n",
    "<ul>\n",
    "    <li><i> Specifically, we resize the image to 4 scales where the shorter dimension (height or\n",
    "        width) is 256, 288, 320 and 352 respectively</i></li>\n",
    "    <li><i>During training, their loss gets added to the total loss of the\n",
    "network with a discount weight (the losses of the auxiliary classifiers were weighted by 0.3). At\n",
    "        inference time, these auxiliary networks are discarded.</i></li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Inception block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Inception block: \n",
    "    #X1 1x1x64 s1  \n",
    "    #X2 1x1x96 s1 => # 3x3x128 s1 : []\n",
    "    #X3 1x1x16 s1 => 5x5x32 s1 \n",
    "    #X4 3x3x192 s1 Max Pool => 1x1x32 s1 \n",
    "    #X1+X2+X3+X4 Concat\n",
    "\n",
    "def inception_block(conv1=[64], conv2=[96,128], conv3=[16,32], conv4=[192, 32], classifier=False): \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
