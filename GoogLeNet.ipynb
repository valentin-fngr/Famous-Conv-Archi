{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's build GoogleNet \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This architecture uses the Inception module, We will explain this module :)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Important notes about the first reading : </h3>\n",
    "<br>\n",
    "<ul> \n",
    "    <li>\n",
    "        <i>\" 1 × 1 convolutions have dual purpose: most critically, they are used mainly as dimension reduction modules to remove computational bottlenecks \"</i>\n",
    "    </li>\n",
    "    <li>\n",
    "        <i>The most straightforward way of improving the performance of deep neural networks is by increasing their size</i>\n",
    "    </li>\n",
    "    <li>\n",
    "        <i>Bigger size typically means a larger number of parameters, which makes the enlarged network more\n",
    "            prone to overfitting</i>\n",
    "    </li>\n",
    "    <li>\n",
    "        <i>Another drawback of uniformly increased network size is the dramatically increased use of computational resources</i>\n",
    "    </li>\n",
    "    <li>\n",
    "        <i> current incarnations of the Inception architecture are restricted to filter sizes 1×1,\n",
    "3×3 and 5×5, however this decision was based more on convenience rather than necessity</i>\n",
    "    <li>\n",
    "        <img src=\"inception.PNG\"/>\n",
    "    </li>\n",
    "    <li>\n",
    "        <i>Besides being used as reductions, they also include the use of rectified linear activation which makes them dual-purpose</i>\n",
    "    </li>\n",
    "    <li>\n",
    "        <i>occasional max-pooling layers with stride 2 to halve the resolution of the grid</i>\n",
    "    </li>\n",
    "    <li>\n",
    "        <img src=\"GoogleNetSummary.PNG\">\n",
    "    </li>\n",
    "    <li>\n",
    "        <i>\n",
    "            We have also\n",
    "used a deeper and wider Inception network, the quality of which was slightly inferior, but adding it\n",
    "to the ensemble seemed to improve the results marginally.\n",
    "        </i>\n",
    "    </li>\n",
    "</ul>\n",
    "\n",
    "<h2>Topology</h2>\n",
    "\n",
    "<ul>\n",
    "    <li><i>size of the receptive field in our network is 224×224 taking RGB color channels with mean subtraction</i></li>\n",
    "    <li><i>“#3×3 reduce” and “#5×5 reduce” stands for the number of 1×1 filters in the reduction</i></li>\n",
    "    <li><i>The network is 22 layers deep when counting only layers with\n",
    "        parameters (or 27 layers if we also count pooling)</i></li>\n",
    "    <li><i>The overall number of layers (independent building blocks) used for the construction of the network is about 100.</i></li>\n",
    "    <li><i>however the use of dropout remained\n",
    "        essential even after removing the fully connected layers</i></li>\n",
    "    <li><i>By adding auxiliary classifiers connected to\n",
    "these intermediate layers, we would expect to encourage discrimination in the lower stages in the\n",
    "classifier, increase the gradient signal that gets propagated back, and provide additional regularization</i></li>\n",
    "    <li>First extra Classifier : </li>\n",
    "    <img src=\"first_classifier.PNG\">\n",
    "    <li>Second extra classifier </li>\n",
    "    <img src=\"extra_classifier2.PNG\">\n",
    "    <li><i>An average pooling layer with 5×5 filter size and stride 3, resulting in an 4×4×512 output\n",
    "        for the (4a), and 4×4×528 for the (4d) stage</i></li>\n",
    "    <li><i>A 1×1 convolution with 128 filters for dimension reduction and rectified linear activation.</i></li>\n",
    "    <li><i>A fully connected layer with 1024 units and rectified linear activation.</i></li>\n",
    "    <li><i>A dropout layer with 70% ratio of dropped outputs</i></li>\n",
    "    <li><i>A linear layer with softmax loss as the classifier (predicting the same 1000 classes as the\n",
    "        main classifier, but removed at inference time).</i></li>\n",
    "    <img src=\"full_GoogleNet.PNG\">\n",
    "</ul>\n",
    "<h2>Training </h2>\n",
    "\n",
    "<ul>\n",
    "    <li><i> Specifically, we resize the image to 4 scales where the shorter dimension (height or\n",
    "        width) is 256, 288, 320 and 352 respectively</i></li>\n",
    "    <li><i>During training, their loss gets added to the total loss of the\n",
    "network with a discount weight (the losses of the auxiliary classifiers were weighted by 0.3). At\n",
    "        inference time, these auxiliary networks are discarded.</i></li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Inception block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inception_block(x,conv1, conv2, conv3, conv4, classifier=False): \n",
    "    x1 = tf.keras.layers.Conv2D(filters=conv1[0], kernel_size=1, strides=1, padding=\"SAME\")(x)\n",
    "    x2 = tf.keras.layers.Conv2D(filters=conv2[0], kernel_size=1, strides=1, padding=\"SAME\")(x)\n",
    "    x2_b = tf.keras.layers.Conv2D(filters=conv2[1], kernel_size=3, strides=1, padding=\"SAME\")(x)\n",
    "    x3 = tf.keras.layers.Conv2D(filters=conv3[0], kernel_size=1, strides=1, padding=\"SAME\")(x)\n",
    "    x3_b = tf.keras.layers.Conv2D(filters=conv3[1], kernel_size=5, strides=1, padding=\"SAME\")(x)\n",
    "    #Max pooling => 1x1\n",
    "    max_pool = tf.keras.layers.MaxPooling2D(strides=1)(x)\n",
    "    x4 = tf.keras.layers.Conv2D(filters=conv4[0], kernel_size=1, strides=1, padding=\"SAME\")(x)\n",
    "    \n",
    "    if classifier: \n",
    "        x5 = tf.keras.layers.AveragePooling2D(pool_size=(5,5),strides=3, padding=\"VALID\")(x)\n",
    "        x5_b = tf.keras.layers.Conv2D(filters=128)\n",
    "        x5_c = tf.keras.layers.Dense(units=1024)\n",
    "    #concatenation \n",
    "    y = tf.keras.layers.Concatenate(axis=3)([x1,x2_b,x3_b,x4])\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### let's test our inception_block with the first inception topology in the paper (3a). According to the paper, we should have a 28x28x256 output tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([None, 28, 28, 256])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we use tf.keras.Input to indicate the input shape\n",
    "A = tf.keras.Input(shape=(28,28,192))\n",
    "# We pass our tensor to the inception block\n",
    "y = inception_block(A, conv1=[64], conv2=[96, 128], conv3=[16,32], conv4=[32])\n",
    "# We compare our output tensor's dimension to the output tensor's dimension in the paper\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
